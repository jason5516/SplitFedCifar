{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6019755a-5e31-4c2b-80ac-c9f5798085e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T15:04:43.209669Z",
     "iopub.status.busy": "2024-06-16T15:04:43.209577Z",
     "iopub.status.idle": "2024-06-16T15:04:44.144608Z",
     "shell.execute_reply": "2024-06-16T15:04:44.144268Z",
     "shell.execute_reply.started": "2024-06-16T15:04:43.209658Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from functools import wraps\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms, utils, datasets\n",
    "from utils.utils import *\n",
    "from utils.training import *\n",
    "# from utils.training_batch import *\n",
    "from utils.model import *\n",
    "from utils.BYOL_models import *\n",
    "from utils.similarity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "146c62cf-2edd-4327-b4f5-b483c0762717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T15:04:44.146003Z",
     "iopub.status.busy": "2024-06-16T15:04:44.145880Z",
     "iopub.status.idle": "2024-06-16T15:04:44.148154Z",
     "shell.execute_reply": "2024-06-16T15:04:44.147964Z",
     "shell.execute_reply.started": "2024-06-16T15:04:44.145994Z"
    }
   },
   "outputs": [],
   "source": [
    "# ('mnist', 'femnist', 'fmnist', 'cifar10', 'cifar100', 'svhn')\n",
    "data_path = \"./data\"\n",
    "dataset = \"cifar10\"\n",
    "# ('noniid-labeldir', 'noniid-label#2', 'noniid-label#3','iid', 'default') default only for femnist\n",
    "partition = \"noniid-label#2\"\n",
    "client_num = 5\n",
    "batch_size = 32\n",
    "test_batch = 250\n",
    "sim_weight = False\n",
    "\n",
    "# Hyperparameters_List (H) = [rounds, number_of_clients, number_of_training_rounds_local, local_batch_size, lr_client, aggregation_frequence]\n",
    "\n",
    "global_epochs = 1000\n",
    "# global_epochs = 3\n",
    "lr = 3e-4\n",
    "dirichlet_beta = 0.4\n",
    "norm = 'bn'\n",
    "# every (avg_freq) epochs doing one aggregation\n",
    "avg_freq = 10\n",
    "# avg_freq = \"exp\"\n",
    "\n",
    "# save_path = f\"./model/SplitFSSLMaxpool_resnet18/resnet18Maxpooling_cifar10_{batch_size}_{partition}_{client_num}\"\n",
    "# save_path = f\"./model/SplitFSSL_BYOL_Avg25times/resnet18Maxpooling_cifar10_{batch_size}_{avg_freq}_{partition}_{client_num}\"\n",
    "# save_path = f\"./model/SplitFSSL_BYOL32_DifAvgtimes_0229/resnet18Maxpooling_{dataset}_{batch_size}_{avg_freq}_{partition}_{client_num}\"\n",
    "save_path = f\"./model/SplitFSSL_BYOL32_similarity/resnet18Maxpooling_sim_weight({sim_weight})_{dataset}_{batch_size}_{avg_freq}_{partition}_{client_num}\"\n",
    "H = [global_epochs, client_num, batch_size, lr, avg_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed800ed7-0a9b-4928-8308-31ee964cede2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T15:04:44.813525Z",
     "iopub.status.busy": "2024-06-16T15:04:44.813377Z",
     "iopub.status.idle": "2024-06-16T15:04:48.595921Z",
     "shell.execute_reply": "2024-06-16T15:04:48.594238Z",
     "shell.execute_reply.started": "2024-06-16T15:04:44.813514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba756754b814d3986976936c063ddd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_125098/1546371873.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnet_dataidx_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_dataidx_map_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraindata_cls_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestdata_cls_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# get shared data idx form test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mshared_data_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SplitfedSSL_logdeep/utils/utils.py\u001b[0m in \u001b[0;36mpartition_data\u001b[0;34m(dataset, datadir, partition, client_num, beta, test_data)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cifar10'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cifar10_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cifar100'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cifar100_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SplitfedSSL_logdeep/utils/utils.py\u001b[0m in \u001b[0;36mload_cifar10_data\u001b[0;34m(datadir)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcifar10_train_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10Pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcifar10_test_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10Pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SplitfedSSL_logdeep/utils/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, dataidxs, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__build_truncated_dataset__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__build_truncated_dataset__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SplitfedSSL_logdeep/utils/datasets.py\u001b[0m in \u001b[0;36m__build_truncated_dataset__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__build_truncated_dataset__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mcifar_dataobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar_dataobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Files already downloaded and verified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" to \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"https\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0m_save_response_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_save_response_content\u001b[0;34m(content, destination, length)\u001b[0m\n\u001b[1;32m     35\u001b[0m ) -> None:\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0m_save_response_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splitfedssl/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if dataset == \"cifar10\":\n",
    "    nclasses = 100\n",
    "else:\n",
    "    nclasses = 10\n",
    "\n",
    "\n",
    "# partition\n",
    "net_dataidx_map, net_dataidx_map_test, traindata_cls_counts, testdata_cls_counts = partition_data(dataset, data_path, partition, client_num)\n",
    "# get shared data idx form test data\n",
    "shared_data_idx = shared_data(dataset, data_path, nclasses, 2500)\n",
    "# get dataloader\n",
    "train_loader_list = []\n",
    "test_loader_list = []\n",
    "for idx in range(client_num):\n",
    "    \n",
    "    dataidxs = net_dataidx_map[idx]\n",
    "    # if net_dataidx_map_test is None:\n",
    "    #     dataidx_test = None \n",
    "    # else:\n",
    "    #     dataidxs_test = net_dataidx_map_test[idx]\n",
    "\n",
    "    train_dl_local, shared_data_loader, train_ds_local, shared_data_ds_local = get_dataloader(dataset, \n",
    "                                                                   data_path, batch_size, test_batch, \n",
    "                                                                   dataidxs, shared_data_idx)\n",
    "    train_loader_list.append(train_dl_local)\n",
    "    # test_loader_list.append(test_dl_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca46fdb-7f2f-4dc9-9143-8acdd53a60d8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-16T15:04:30.621022Z",
     "iopub.status.idle": "2024-06-16T15:04:30.621107Z",
     "shell.execute_reply": "2024-06-16T15:04:30.621068Z",
     "shell.execute_reply.started": "2024-06-16T15:04:30.621064Z"
    }
   },
   "outputs": [],
   "source": [
    "shared_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e62922-5a58-41ab-9686-061864a6e95e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-16T15:04:30.621374Z",
     "iopub.status.idle": "2024-06-16T15:04:30.621459Z",
     "shell.execute_reply": "2024-06-16T15:04:30.621419Z",
     "shell.execute_reply.started": "2024-06-16T15:04:30.621415Z"
    }
   },
   "outputs": [],
   "source": [
    "cls_count = np.zeros([10,client_num], dtype=int)\n",
    "for k in range(client_num):\n",
    "    for i in traindata_cls_counts[k]:\n",
    "        cls_count[i][k] = traindata_cls_counts[k][i]\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cls_count, yticklabels=class_names, square=False,annot=True,fmt='d',linecolor='white',cmap='Blues',linewidths=1.5)\n",
    "plt.xlabel('client',fontsize=20)\n",
    "plt.ylabel('class',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a99ffb-6f36-4833-8a64-68f4a80b2346",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-16T15:04:30.621707Z",
     "iopub.status.idle": "2024-06-16T15:04:30.621792Z",
     "shell.execute_reply": "2024-06-16T15:04:30.621753Z",
     "shell.execute_reply.started": "2024-06-16T15:04:30.621749Z"
    }
   },
   "outputs": [],
   "source": [
    "net = ResNet18()\n",
    "client_model = BYOL_Client()\n",
    "client_model_test = BYOL_Client()\n",
    "server_model = BYOL_Server()\n",
    "server_model.cuda()\n",
    "\n",
    "client_weights = [1/5 for i in range(client_num)]\n",
    "client_simweights = [[1 for i in range(client_num)] for i in range(client_num)]\n",
    "client_models = [copy.deepcopy(client_model).cuda() for idx in range(client_num)]\n",
    "client_tempmodels = [copy.deepcopy(client_model) for idx in range(client_num)]\n",
    "\n",
    "# server_models = [copy.deepcopy(server_model).cuda() for idx in range(client_num)]\n",
    "\n",
    "optimizer_server = torch.optim.Adam(server_model.parameters(), lr = H[3]) \n",
    "optimizer_clients = [torch.optim.Adam(client_models[i].parameters(), lr = H[3]) for i in range(len(client_models))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265f961-8bdb-4603-906e-f00de80d6009",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-16T15:04:30.622091Z",
     "iopub.status.idle": "2024-06-16T15:04:30.622179Z",
     "shell.execute_reply": "2024-06-16T15:04:30.622140Z",
     "shell.execute_reply.started": "2024-06-16T15:04:30.622136Z"
    }
   },
   "outputs": [],
   "source": [
    "# epoch = 0 \n",
    "checkpath = save_path + \"/checkpoint.pth.tar\" \n",
    "checkpoint = torch.load(checkpath)\n",
    "epoch = checkpoint['glepoch']\n",
    "print(epoch)\n",
    "optimizer_server.load_state_dict(checkpoint['optimizer'][0])\n",
    "if sim_weight:\n",
    "    for clientidx in range(client_num):\n",
    "        client_models[clientidx].online_encoder.load_state_dict(checkpoint['state_dict'][clientidx])\n",
    "else:\n",
    "    for localmodel in client_models:\n",
    "        localmodel.online_encoder.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for clientidx in range(client_num):\n",
    "    optimizer_clients[clientidx].load_state_dict(checkpoint['optimizer'][clientidx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5e381-da78-4fc0-9bd8-aac7aac4cc28",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-16T15:04:30.622488Z",
     "iopub.status.idle": "2024-06-16T15:04:30.622600Z",
     "shell.execute_reply": "2024-06-16T15:04:30.622545Z",
     "shell.execute_reply.started": "2024-06-16T15:04:30.622538Z"
    }
   },
   "outputs": [],
   "source": [
    "def training(client_models, server_model, optimizer_server, optimizer_clients, rounds, batch_size, avg_freq):\n",
    "   \n",
    "    # training loss\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    best_accuracy = 0\n",
    "    avg_times = 0\n",
    "    # measure time\n",
    "    start = time.time()\n",
    "    pre_clients_features = None\n",
    "    feature_sim_dict = {}\n",
    "    \n",
    "    \n",
    "    writer = SummaryWriter(f'logs/SplitFSSL_BYOL32_similarity_feature0423/resnet18Maxpooling_cifar10_{batch_size}_{avg_freq}_{partition}_{client_num}')\n",
    "    # writer = SummaryWriter(f'logs/SplitFSSL_BYOL_Avg25times/resnet18Maxpooling_cifar10_{batch_size}_{avg_freq}_{partition}_{client_num}')\n",
    "    global_step = 0\n",
    "    for curr_round in range(epoch, rounds+1):\n",
    "        metrics = defaultdict(list)\n",
    "        print(f\"Global Round:\", curr_round)\n",
    "        w, local_loss = [], []\n",
    "        \n",
    "        num_batch = 0\n",
    "        for i in train_loader_list:\n",
    "            if num_batch < len(i):\n",
    "                num_batch = len(i)\n",
    "                \n",
    "        train_iter = []\n",
    "        for i in train_loader_list:\n",
    "            train_iter.append(iter(i))\n",
    "            \n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        p_bar = tqdm(range(num_batch))\n",
    "\n",
    "        # 聚合頻率參數成指數成長\n",
    "        # alpha = expavg_times(curr_round)\n",
    "        # 聚合頻率參數線性數成長\n",
    "        # alpha = linear_growth(curr_round)\n",
    "        # 聚合頻率參數成固定參數\n",
    "        alpha = avg_freq\n",
    "        \n",
    "        for batch in range(num_batch):\n",
    "            # print(\"0>\", time.time() - start)\n",
    "            optimizer_zero_grads(optimizer_server, optimizer_clients)\n",
    "            \n",
    "            online_proj_one_list = [None for _ in range(client_num)]\n",
    "            online_proj_two_list = [None for _ in range(client_num)]\n",
    "            target_proj_one_list = [None for _ in range(client_num)]\n",
    "            target_proj_two_list = [None for _ in range(client_num)]\n",
    "\n",
    "            # client forward\n",
    "            # select 5 client to join training\n",
    "            s_clients = []\n",
    "            s_clients = range(client_num)\n",
    "            # s_clients = random.sample(range(client_num), 6)\n",
    "            # print(\"1>\", time.time() - start)\n",
    "            for i, client_id in enumerate(s_clients):\n",
    "                # print(\"Client: \",i)\n",
    "                # Compute a local update\n",
    "                # print(i, \"0>\", time.time() - start)\n",
    "                img1, img2, _ = next_data_batch(train_iter[client_id], train_loader_list[client_id])\n",
    "                \n",
    "                img1 = img1.cuda()\n",
    "                img2 = img2.cuda()\n",
    "                \n",
    "                data_time.update(time.time() - start)\n",
    "                # print(i, \"1>\", time.time() - start)\n",
    "                # pass to client model\n",
    "                # print(\"pass to client model\")\n",
    "                client_models[client_id].train()\n",
    "                # print(i, \"2>\", time.time() - start)\n",
    "                online_proj_one, online_proj_two, target_proj_one, target_proj_two = client_models[client_id](img1, img2)\n",
    "                # print(i, \"3>\", time.time() - start)\n",
    "                \n",
    "                # store representations\n",
    "                online_proj_one_list[i] = online_proj_one\n",
    "                online_proj_two_list[i] = online_proj_two\n",
    "                target_proj_one_list[i] = target_proj_one\n",
    "                target_proj_two_list[i] = target_proj_two\n",
    "                  \n",
    "\n",
    "            \n",
    "            # stack representations\n",
    "            stack_online_proj_one = torch.cat(online_proj_one_list, dim = 0)\n",
    "            stack_online_proj_two = torch.cat(online_proj_two_list, dim = 0)\n",
    "            stack_target_proj_one = torch.cat(target_proj_one_list, dim = 0)\n",
    "            stack_target_proj_two = torch.cat(target_proj_two_list, dim = 0)\n",
    "\n",
    "            # print(\">\", time.time() - start)\n",
    "            stack_online_proj_one, stack_online_proj_two, stack_target_proj_one, stack_target_proj_two = stack_online_proj_one.cuda(), stack_online_proj_two.cuda(), stack_target_proj_one.cuda(), stack_target_proj_two.cuda()\n",
    "            \n",
    "            # server computes\n",
    "            # print(\"server computes\")\n",
    "            online_proj_one_grad, online_proj_two_grad, loss = train_server(stack_online_proj_one.detach(), stack_online_proj_two.detach(), stack_target_proj_one, stack_target_proj_two, server_model)\n",
    "            local_loss.append((loss.item()))\n",
    "            optimizer_server.step()\n",
    "            # print(time.time() - start)\n",
    "            # distribute gradients to clients\n",
    "            # online_proj_one_grad, online_proj_two_grad = online_proj_one_grad.cpu(), online_proj_two_grad.cpu()\n",
    "            gradient_dict_one = {key: [] for key in range(client_num)}\n",
    "            gradient_dict_two = {key: [] for key in range(client_num)}\n",
    "            \n",
    "            for j in range(client_num):\n",
    "                gradient_dict_one[j] = online_proj_one_grad[j*batch_size:(j+1)*batch_size, :]\n",
    "                gradient_dict_two[j] = online_proj_two_grad[j*batch_size:(j+1)*batch_size, :]\n",
    "                \n",
    "            \n",
    "            for i, client_id in enumerate(s_clients):\n",
    "                online_proj_one_list[i].backward(gradient_dict_one[i])\n",
    "                online_proj_two_list[i].backward(gradient_dict_two[i])\n",
    "                optimizer_clients[client_id].step()\n",
    "                client_models[client_id].update_moving_average()\n",
    "            \n",
    "            # if (batch+1)%10 == 0:\n",
    "            #     print(f\"Step [{batch}/{num_batch}]:\\tLoss: {loss.item()}\")\n",
    "            \n",
    "            del img1, img2\n",
    "            writer.add_scalar(\"Loss/train_step\", loss, global_step)\n",
    "            metrics[\"Loss/train\"].append(loss.item())\n",
    "            global_step += 1\n",
    "            \n",
    "            batch_time.update(time.time() - start)\n",
    "            start = time.time()\n",
    "            #=======================================set p_bar description=======================================================\n",
    "            p_bar.set_description(\"Train Epoch: {epoch}/{epochs:4}. Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. alpha: {ep_alpha}. Batch: {bt:.3f}s. Loss: {loss:.4f}.\".format(\n",
    "                    epoch=curr_round,\n",
    "                    epochs=rounds,\n",
    "                    batch=batch + 1,\n",
    "                    iter=num_batch,\n",
    "                    data=data_time.avg,\n",
    "                    ep_alpha = alpha,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=loss.item()))\n",
    "            p_bar.update()\n",
    "            #=======================================set p_bar description=======================================================\n",
    "            # in 32 batch size will have 250 batches, if aggregate per 10 batches will have 25 aggerations in one epoch\n",
    "            # in 64 batch size will have 125 batches, if aggregate per 5 batches will have 25 aggerations in one epoch\n",
    "            if batch == num_batch - 1 or ((batch+1) % alpha == 0):\n",
    "                \n",
    "                # calculate similarity matrix\n",
    "                if avg_times % 10 == 0:\n",
    "                    clients_similarity, mat_sim, clients_features = similarity_mat(save_path, curr_round, avg_times, s_clients, client_models, shared_data_loader, \n",
    "                                                            nclasses=512, nsamples=2500)\n",
    "                    breakpoint()\n",
    "                    \n",
    "                    if pre_clients_features == None:\n",
    "                        pre_clients_features = clients_features\n",
    "                    else:\n",
    "                        # 計算每個client與上次features的相似度\n",
    "                        # breakpoint()\n",
    "                        feature_sim_dict[curr_round] = cos_similarity(pre_clients_features, clients_features)\n",
    "                        pre_clients_features = clients_features\n",
    "                        print(f\"epoch {curr_round} similarity : {feature_sim_dict[curr_round]}\")\n",
    "                    # 用softmax運算\n",
    "                    # for i in range(client_num):\n",
    "                    #     w = clients_similarity[i]\n",
    "                    #     weight = torch.softmax(torch.tensor(w), dim=0)\n",
    "                    #     client_simweights[i] = weight\n",
    "                        \n",
    "                    for i in range(client_num):\n",
    "                        weights_sum = np.sum(clients_similarity[i])\n",
    "                        for k, w in enumerate(clients_similarity[i]):\n",
    "                            client_simweights[i][k] = w / weights_sum\n",
    "\n",
    "                    # for idx, w in enumerate(client_simweights):\n",
    "                    #     print(f\"client {idx} weight : {w}, sum : {np.sum(w)}\")\n",
    "            \n",
    "                # print(\"aggregate batch\", batch)\n",
    "                avg_times += 1\n",
    "                with torch.no_grad():\n",
    "                    # aggregate client models\n",
    "                    # for key, param in model.named_parameters():\n",
    "                    #     if param.requires_grad:\n",
    "                    for key in client_model.state_dict().keys():\n",
    "                        if \"running\" in key or \"num_batches\" in key:\n",
    "                            continue\n",
    "                        if sim_weight:\n",
    "                            # 計算每個client的聚合權重\n",
    "                            for client_idx in s_clients:\n",
    "                                temp = torch.zeros_like(client_model.state_dict()[key]).to('cuda')\n",
    "                                for i in s_clients:\n",
    "                                    temp += client_simweights[client_idx][i] * client_models[i].state_dict()[key]                        \n",
    "                                client_tempmodels[client_idx].state_dict()[key].data.copy_(temp)\n",
    "                            for client_idx in range(len(client_models)):\n",
    "                                client_models[client_idx].state_dict()[key].data.copy_(client_tempmodels[client_idx].state_dict()[key])\n",
    "                            \n",
    "                        \n",
    "                        else:\n",
    "                            temp = torch.zeros_like(client_model.state_dict()[key]).to('cuda')\n",
    "                            for client_idx in s_clients:\n",
    "                                temp += client_weights[client_idx] * client_models[client_idx].state_dict()[key]                        \n",
    "                            client_model.state_dict()[key].data.copy_(temp)\n",
    "                            for client_idx in range(len(client_models)):\n",
    "                                # temp = 0.8 * client_model.state_dict()[key].to('cuda') + 0.2 * client_models[client_idx].state_dict()[key]\n",
    "                                # client_models[client_idx].state_dict()[key].data.copy_(temp)\n",
    "                                client_models[client_idx].state_dict()[key].data.copy_(client_model.state_dict()[key])\n",
    "                            \n",
    "    \n",
    "        \n",
    "        p_bar.close()\n",
    "        # scheduler_server.step()\n",
    "        for k, v in metrics.items():\n",
    "            writer.add_scalar(k, np.array(v).mean(), curr_round)\n",
    "\n",
    "\n",
    "        # loss\n",
    "        loss_avg = sum(local_loss) / len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "        if curr_round % 5 == 0:\n",
    "            optimizer_dict = []\n",
    "            optimizer_dict.append(optimizer_server.state_dict())            \n",
    "            for client_idx in range(client_num):\n",
    "                optimizer_dict.append(optimizer_clients[client_idx].state_dict())\n",
    "            if sim_weight:\n",
    "                state_dict = []\n",
    "                for cid in range(client_num):\n",
    "                    state_dict.append(client_models[cid].online_encoder.cpu().state_dict())\n",
    "                    client_models[cid].to('cuda')\n",
    "            else:\n",
    "                state_dict = client_model.online_encoder.cpu().state_dict()\n",
    "            save_checkpoint({\n",
    "                'glepoch': curr_round+1,\n",
    "                'state_dict': state_dict,\n",
    "                'optimizer': optimizer_dict,\n",
    "                'feature_dict': feature_sim_dict,\n",
    "            }, save_path)\n",
    "        if curr_round % 100 == 0:\n",
    "            if sim_weight:\n",
    "                if not os.path.exists(save_path + f\"/epoch_{curr_round}\"):\n",
    "                    os.makedirs(save_path + f\"/epoch_{curr_round}\")\n",
    "                for cid in range(client_num):\n",
    "                    torch.save(client_models[cid].online_encoder.cpu().state_dict(), save_path + f\"/epoch_{curr_round}/client_{cid}.pt\")\n",
    "                    client_models[cid].to('cuda')      \n",
    "            else:\n",
    "                torch.save(client_model.online_encoder.cpu().state_dict(), save_path + f\"_{curr_round}_epoch.pt\")\n",
    "        \n",
    "        \n",
    "        print(f\"Global round: {curr_round} | Average loss: {loss_avg}\")\n",
    "        # print('best_accuracy:', best_accuracy, '---Round:', curr_round, '---lr', lr, '----localEpocs--', E)\n",
    "\n",
    "    end = time.time()\n",
    "   \n",
    "    print(\"Training Done!\")\n",
    "    print(\"Total time taken to Train: {}\".format(end - start))\n",
    "    print(f\"Total average times : {avg_times}\")\n",
    "\n",
    "    return client_models, client_model, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b93bb8-f32b-49c8-92bb-94958603dd6e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-16T15:04:30.622947Z",
     "iopub.status.idle": "2024-06-16T15:04:30.623029Z",
     "shell.execute_reply": "2024-06-16T15:04:30.622990Z",
     "shell.execute_reply.started": "2024-06-16T15:04:30.622987Z"
    }
   },
   "outputs": [],
   "source": [
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590367d-8fcf-47bd-80b6-8dbc4ffff96c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-16T15:04:30.623224Z",
     "iopub.status.idle": "2024-06-16T15:04:30.623305Z",
     "shell.execute_reply": "2024-06-16T15:04:30.623267Z",
     "shell.execute_reply.started": "2024-06-16T15:04:30.623264Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client_models, client_model, train_loss = training(client_models, server_model, optimizer_server, optimizer_clients, H[0], H[2], H[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcae480-59d1-455e-a52a-2c23e442aec5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-16T15:04:30.623526Z",
     "iopub.status.idle": "2024-06-16T15:04:30.623605Z",
     "shell.execute_reply": "2024-06-16T15:04:30.623568Z",
     "shell.execute_reply.started": "2024-06-16T15:04:30.623564Z"
    }
   },
   "outputs": [],
   "source": [
    "if sim_weight:\n",
    "    if not os.path.exists(save_path + f\"/final\"):\n",
    "        os.makedirs(save_path + f\"/final\")\n",
    "    for cid in range(client_num):\n",
    "        torch.save(client_models[cid].online_encoder.cpu().state_dict(), save_path + f\"/final/client_{cid}.pt\")\n",
    "else:\n",
    "    torch.save(client_model.online_encoder.cpu().state_dict(), save_path + \"_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1619dc-d7c1-4ee0-9182-ee070f70ab5f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-16T15:04:30.623888Z",
     "iopub.status.idle": "2024-06-16T15:04:30.623971Z",
     "shell.execute_reply": "2024-06-16T15:04:30.623933Z",
     "shell.execute_reply.started": "2024-06-16T15:04:30.623929Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot training losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-splitfedssl]",
   "language": "python",
   "name": "conda-env-.conda-splitfedssl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
